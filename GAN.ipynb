{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c60c62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     19\u001b[0m     sketches_generator \u001b[38;5;241m=\u001b[39m data_generator\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     20\u001b[0m         sketches_dir,\n\u001b[0;32m     21\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m real_images_generator, sketches_generator\n\u001b[1;32m---> 28\u001b[0m real_images_data \u001b[38;5;241m=\u001b[39m preprocess_data(real_images_data)\n\u001b[0;32m     29\u001b[0m sketches_data \u001b[38;5;241m=\u001b[39m preprocess_data(sketches_data)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Preprocessing function (same as before)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(real_images_dir, sketches_dir, batch_size, image_size):\n",
    "    # Create image data generators for real-time face images and sketches\n",
    "    data_generator = ImageDataGenerator(rescale=1./255)\n",
    "    real_images_generator = data_generator.flow_from_directory(\n",
    "        real_images_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        seed=42\n",
    "    )\n",
    "    sketches_generator = data_generator.flow_from_directory(\n",
    "        sketches_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        seed=42\n",
    "    )\n",
    "    return real_images_generator, sketches_generator\n",
    "\n",
    "real_images_data = preprocess_data(real_images_data)\n",
    "sketches_data = preprocess_data(sketches_data)\n",
    "\n",
    "# Preprocessing function (same as before)\n",
    "def preprocess_images(images):\n",
    "    return (images.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Define the GAN model (same as before)\n",
    "def build_gan(generator, discriminator, learning_rate):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.5))\n",
    "    return gan\n",
    "\n",
    "# Define training function (same as before)\n",
    "def train_gan(generator, discriminator, gan, real_images_generator, sketches_generator, epochs, batch_size):\n",
    "    steps_per_epoch = min(real_images_generator.n // batch_size, sketches_generator.n // batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        real_images_iterator = iter(real_images_generator)\n",
    "        sketches_iterator = iter(sketches_generator)\n",
    "        for step in range(steps_per_epoch):\n",
    "            real_images_batch = preprocess_images(next(real_images_iterator))\n",
    "            sketches_batch = preprocess_images(next(sketches_iterator))\n",
    "\n",
    "            # Generate random noise for the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Concatenate real images and generated images\n",
    "            X = np.concatenate([real_images_batch, sketches_batch])\n",
    "            # Labels for real and generated images\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9  # Label smoothing\n",
    "\n",
    "            # Train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "def build_model(hp):\n",
    "    # Define hyperparameters to tune\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n",
    "\n",
    "    # Build the GAN model with tunable hyperparameters\n",
    "    gan = build_gan(generator, discriminator, learning_rate=hp_learning_rate)\n",
    "    \n",
    "    return gan\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # Load data\n",
    "    real_images_dir = 'path/to/real/images'\n",
    "    sketches_dir = 'path/to/sketches'\n",
    "    batch_size = 32\n",
    "    image_size = (64, 64)  # Adjust image size as needed\n",
    "    real_images_generator, sketches_generator = load_data(real_images_dir, sketches_dir, batch_size, image_size)\n",
    "\n",
    "    # Define input shape\n",
    "    input_shape = image_size + (3,)  # Assuming RGB images\n",
    "\n",
    "    # Build generator and discriminator (you may adjust these architectures)\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(256, input_shape=(100,)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1024),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(np.prod(input_shape), activation='tanh'),\n",
    "        layers.Reshape(input_shape)\n",
    "    ])\n",
    "\n",
    "    discriminator = models.Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Define the search strategy and start the hyperparameter search\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,  # Adjust as needed\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name='gan_hyperparameter_tuning')\n",
    "\n",
    "    tuner.search_space_summary()  # Optional: Print the search space summary\n",
    "\n",
    "    # Start the hyperparameter search using the previously defined data generators\n",
    "    tuner.search(real_images_data, validation_data=(sketches_data, sketches_data), epochs=100)\n",
    "\n",
    "    # Retrieve the best hyperparameters found during the search\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Best hyperparameters: {best_hps}\")\n",
    "\n",
    "    # Retrieve and save the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.save('best_generator_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b1fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ddff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973e5953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Obtaining dependency information for keras-tuner from https://files.pythonhosted.org/packages/2b/39/21f819fcda657c37519cf817ca1cd03a8a025262aad360876d2a971d38b3/keras_tuner-1.4.6-py3-none-any.whl.metadata\n",
      "  Downloading keras_tuner-1.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from keras-tuner) (2.15.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: requests in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from keras-tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paivi\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2023.11.17)\n",
      "Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.9 kB ? eta -:--:--\n",
      "   ------------ -------------------------- 41.0/128.9 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 61.4/128.9 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ---- 112.6/128.9 kB 819.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 128.9/128.9 kB 847.7 kB/s eta 0:00:00\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647239de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real images:  1000\n",
      "Number of sketches:  1000\n",
      "Reloading Tuner from my_dir\\gan_hyperparameter_tuning\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'leaky_relu'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001, 1e-05], 'ordered': True}\n",
      "batch_size (Choice)\n",
      "{'default': 16, 'conditions': [], 'values': [16, 32, 64], 'ordered': True}\n",
      "units (Choice)\n",
      "{'default': 128, 'conditions': [], 'values': [128, 256, 512], 'ordered': True}\n",
      "hidden_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "leaky_relu        |relu              |activation\n",
      "1e-05             |0.001             |learning_rate\n",
      "64                |32                |batch_size\n",
      "256               |512               |units\n",
      "3                 |1                 |hidden_layers\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 232, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 164, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 155, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\ipykernel_5952\\126285426.py\", line 90, in build_gan\n",
      "    generator.add(layers.Reshape((64, 64, 3)))  # Adjust dimensions as needed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py\", line 118, in _fix_unknown_dimension\n",
      "    raise ValueError(msg)\n",
      "ValueError: Exception encountered when calling layer \"reshape\" (type Reshape).\n",
      "\n",
      "total size of new array must be unchanged, input_shape = [100], output_shape = [64, 64, 3]\n",
      "\n",
      "Call arguments received by layer \"reshape\" (type Reshape):\n",
      "  • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\ipykernel_5952\\126285426.py\", line 90, in build_gan\n    generator.add(layers.Reshape((64, 64, 3)))  # Adjust dimensions as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py\", line 118, in _fix_unknown_dimension\n    raise ValueError(msg)\nValueError: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [100], output_shape = [64, 64, 3]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 176\u001b[0m\n\u001b[0;32m    173\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()  \u001b[38;5;66;03m# Optional: Print the search space summary\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Start the hyperparameter search using the preprocessed data\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(real_images_data, validation_data\u001b[38;5;241m=\u001b[39m(sketches_data, sketches_data), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Retrieve the best hyperparameters found during the search\u001b[39;00m\n\u001b[0;32m    179\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mend_trial(trial)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[1;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[0;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:586\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_consecutive_failures()\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:543\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    541\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    547\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\ipykernel_5952\\126285426.py\", line 90, in build_gan\n    generator.add(layers.Reshape((64, 64, 3)))  # Adjust dimensions as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py\", line 118, in _fix_unknown_dimension\n    raise ValueError(msg)\nValueError: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [100], output_shape = [64, 64, 3]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from PIL import Image\n",
    "import os\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(images_data):\n",
    "    # Flatten the images to a vector of length 100\n",
    "    flattened_images = images_data.reshape(images_data.shape[0], -1)  # Reshape to (None, 100)\n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    normalized_images = flattened_images.astype('float32') / 255.0\n",
    "    return normalized_images\n",
    "\n",
    "\n",
    "def convert_to_jpg(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            continue  # Skip files already in .jpg format\n",
    "        try:\n",
    "            img = Image.open(os.path.join(directory, filename))\n",
    "            new_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "            img.save(os.path.join(directory, new_filename))\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {filename}: {e}\")\n",
    "# Load and preprocess data\n",
    "\n",
    "\n",
    "def load_data(real_images_dir, sketches_dir, batch_size, image_size):\n",
    "    # Function to load and preprocess images\n",
    "    def preprocess_images(image_paths, target_size):\n",
    "        images = []\n",
    "        for path in image_paths:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, target_size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            img = img.astype(np.float32) / 255.0        # Normalize pixel values\n",
    "        # Flatten the image to a vector of length 100\n",
    "            img = img.flatten()\n",
    "            images.append(img)\n",
    "        return np.array(images)\n",
    "\n",
    "\n",
    "\n",
    "    # Load real images\n",
    "    real_images_paths = [os.path.join(real_images_dir, filename) for filename in os.listdir(real_images_dir)]\n",
    "    real_images = preprocess_images(real_images_paths, image_size)\n",
    "\n",
    "    # Load sketches\n",
    "    sketches_paths = [os.path.join(sketches_dir, filename) for filename in os.listdir(sketches_dir)]\n",
    "    sketches = preprocess_images(sketches_paths, image_size)\n",
    "\n",
    "    print(\"Number of real images: \", len(real_images))\n",
    "    print(\"Number of sketches: \", len(sketches))\n",
    "\n",
    "    return real_images, sketches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_images(images):\n",
    "    return (images.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(hp):\n",
    "    # Define hyperparameters to tune\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'leaky_relu'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n",
    "    hp_units = hp.Choice('units', values=[128, 256, 512])\n",
    "    hp_hidden_layers = hp.Int('hidden_layers', min_value=1, max_value=3)\n",
    "\n",
    "    # Build the generator\n",
    "    generator = models.Sequential()\n",
    "    generator.add(layers.Dense(hp_units, input_shape=(100,), activation=hp_activation))\n",
    "    generator.add(layers.BatchNormalization())\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        generator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "        generator.add(layers.BatchNormalization())\n",
    "    generator.add(layers.Dense(100, activation='tanh'))  # Output shape should be (100,)\n",
    "\n",
    "    generator.add(layers.Reshape((64, 64, 3)))  # Adjust dimensions as needed\n",
    "\n",
    "\n",
    "    # Build the discriminator\n",
    "    discriminator = models.Sequential()\n",
    "    discriminator.add(layers.Flatten(input_shape=input_shape))\n",
    "    discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    discriminator.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Compile GAN\n",
    "    gan_input = layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5))\n",
    "\n",
    "    return gan\n",
    "\n",
    "# Define training function\n",
    "def train_gan(generator, discriminator, gan, real_images_generator, sketches_generator, epochs, batch_size):\n",
    "    steps_per_epoch = min(real_images_generator.n // batch_size, sketches_generator.n // batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        real_images_iterator = iter(real_images_generator)\n",
    "        sketches_iterator = iter(sketches_generator)\n",
    "        for step in range(steps_per_epoch):\n",
    "            real_images_batch = preprocess_images(next(real_images_iterator))\n",
    "            sketches_batch = preprocess_images(next(sketches_iterator))\n",
    "\n",
    "            # Generate random noise for the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Concatenate real images and generated images\n",
    "            X = np.concatenate([real_images_batch, sketches_batch])\n",
    "            # Labels for real and generated images\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9  # Label smoothing\n",
    "\n",
    "            # Train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')\n",
    "\n",
    "# Main function\n",
    "# Load data\n",
    "real_images_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Real\"\n",
    "sketches_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Sketch_1\"\n",
    "\n",
    "batch_size = 32\n",
    "image_size = (64, 64)  # Adjust image size as needed\n",
    "real_images_data, sketches_data = load_data(real_images_dir, sketches_dir, batch_size, image_size)\n",
    "\n",
    "# Preprocess the loaded data\n",
    "# Preprocess the loaded data\n",
    "real_images_data = preprocess_data(real_images_data)\n",
    "sketches_data = preprocess_data(sketches_data)\n",
    "\n",
    "\n",
    "# Define input shape\n",
    "input_shape = image_size + (3,)  # Assuming RGB images\n",
    "\n",
    "# Define the search strategy and start the hyperparameter search\n",
    "tuner = RandomSearch(\n",
    "    build_gan,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,  # Adjust as needed\n",
    "    executions_per_trial=5,\n",
    "    directory='my_dir',\n",
    "    project_name='gan_hyperparameter_tuning')\n",
    "\n",
    "tuner.search_space_summary()  # Optional: Print the search space summary\n",
    "\n",
    "# Start the hyperparameter search using the preprocessed data\n",
    "tuner.search(real_images_data, validation_data=(sketches_data, sketches_data), epochs=100)\n",
    "\n",
    "# Retrieve the best hyperparameters found during the search\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps}\")\n",
    "\n",
    "# Retrieve and save the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('best_generator_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1dd4b",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495473af",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25aa30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real images: 1000\n",
      "Number of sketches: 1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 158\u001b[0m\n\u001b[0;32m    147\u001b[0m     sketches_data \u001b[38;5;241m=\u001b[39m preprocess_data(sketches)\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# Define the search strategy and start the hyperparameter search\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     tuner \u001b[38;5;241m=\u001b[39m RandomSearch(\n\u001b[0;32m    151\u001b[0m         build_gan,\n\u001b[0;32m    152\u001b[0m         objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    153\u001b[0m         max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    154\u001b[0m         executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    155\u001b[0m         directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_dir\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    156\u001b[0m         project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgan_hyperparameter_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    157\u001b[0m         hyperparameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaky_relu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1e-06\u001b[39m, \u001b[38;5;241m1e-05\u001b[39m]),\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]),\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m]),\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m    163\u001b[0m     }\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    165\u001b[0m     tuner\u001b[38;5;241m.\u001b[39msearch(real_images_data, validation_data\u001b[38;5;241m=\u001b[39m(sketches_data, sketches_data), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Retrieve the best hyperparameters found during the search\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(images_data):\n",
    "    # Flatten the images to a vector of length 100\n",
    "    flattened_images = images_data.reshape(images_data.shape[0], -1)  # Reshape to (None, 100)\n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    normalized_images = flattened_images.astype('float32') / 255.0\n",
    "    return normalized_images\n",
    "\n",
    "# Function to convert images to JPG format\n",
    "def convert_to_jpg(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            continue  # Skip files already in .jpg format\n",
    "        try:\n",
    "            img = Image.open(os.path.join(directory, filename))\n",
    "            new_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "            img.save(os.path.join(directory, new_filename))\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {filename}: {e}\")\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_data(real_images_dir, sketches_dir, batch_size, image_size):\n",
    "    # Function to preprocess images\n",
    "    def preprocess_images(image_paths, target_size):\n",
    "        images = []\n",
    "        for path in image_paths:\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, target_size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            img = img.astype(np.float32) / 255.0        # Normalize pixel values\n",
    "            images.append(img)\n",
    "        return np.array(images)\n",
    "\n",
    "    # Load real images\n",
    "    real_images_paths = [os.path.join(real_images_dir, filename) for filename in os.listdir(real_images_dir)]\n",
    "    real_images = preprocess_images(real_images_paths, image_size)\n",
    "\n",
    "    # Load sketches\n",
    "    sketches_paths = [os.path.join(sketches_dir, filename) for filename in os.listdir(sketches_dir)]\n",
    "    sketches = preprocess_images(sketches_paths, image_size)\n",
    "\n",
    "    print(\"Number of real images:\", len(real_images))\n",
    "    print(\"Number of sketches:\", len(sketches))\n",
    "\n",
    "    return real_images, sketches\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Define the GAN model\n",
    "# Define the GAN model\n",
    "def build_gan(hp):\n",
    "    # Define hyperparameters to tune\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'leaky_relu'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n",
    "    hp_units = hp.Choice('units', values=[128, 256, 512])\n",
    "    hp_hidden_layers = hp.Int('hidden_layers', min_value=1, max_value=3)\n",
    "\n",
    "    # Build the generator\n",
    "    generator = models.Sequential()\n",
    "    generator.add(layers.Dense(hp_units, input_shape=(100,), activation=hp_activation))\n",
    "    generator.add(layers.BatchNormalization())\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        generator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "        generator.add(layers.BatchNormalization())\n",
    "    generator.add(layers.Dense(100, activation='tanh'))  # Output shape should be (100,)\n",
    "\n",
    "    generator.add(layers.Reshape((10, 10, 1)))  # Adjust dimensions as needed\n",
    "\n",
    "    # Build the discriminator\n",
    "    discriminator = models.Sequential()\n",
    "    discriminator.add(layers.Flatten(input_shape=(10, 10, 1)))  # Adjust input shape accordingly\n",
    "    discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    discriminator.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Compile GAN\n",
    "    gan_input = layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5))\n",
    "\n",
    "    return gan\n",
    "\n",
    "\n",
    "# Define training function\n",
    "def train_gan(generator, discriminator, gan, real_images, sketches, epochs, batch_size):\n",
    "    real_images_generator = ImageDataGenerator().flow(real_images, batch_size=batch_size, shuffle=True)\n",
    "    sketches_generator = ImageDataGenerator().flow(sketches, batch_size=batch_size, shuffle=True)\n",
    "    steps_per_epoch = min(len(real_images) // batch_size, len(sketches) // batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(steps_per_epoch):\n",
    "            real_images_batch = next(real_images_generator)\n",
    "            sketches_batch = next(sketches_generator)\n",
    "\n",
    "            # Generate random noise for the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            generated_images = generator.predict(noise)\n",
    "\n",
    "            # Concatenate real images and generated images\n",
    "            X = np.concatenate([real_images_batch, sketches_batch])\n",
    "            # Labels for real and generated images\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9  # Label smoothing\n",
    "\n",
    "            # Train the discriminator\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train the generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    real_images_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Real\"\n",
    "    sketches_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Sketch_1\"\n",
    "    batch_size = 32\n",
    "    image_size = (64, 64)  # Adjust image size as needed\n",
    "    real_images, sketches = load_data(real_images_dir, sketches_dir, batch_size, image_size)\n",
    "\n",
    "    # Preprocess the loaded data\n",
    "    real_images_data = preprocess_data(real_images)\n",
    "    sketches_data = preprocess_data(sketches)\n",
    "\n",
    "    # Define the search strategy and start the hyperparameter search\n",
    "    tuner = RandomSearch(\n",
    "        build_gan,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name='gan_hyperparameter_tuning',\n",
    "        hyperparameters={\n",
    "        'activation': hp.Choice('activation', values=['leaky_relu', 'relu']),\n",
    "        'learning_rate': hp.Choice('learning_rate', values=[1e-06, 1e-05]),\n",
    "        'batch_size': hp.Choice('batch_size', values=[128, 256]),\n",
    "        'units': hp.Choice('units', values=[512, 1024]),\n",
    "        'hidden_layers': hp.Choice('hidden_layers', values=[2, 3])\n",
    "    }\n",
    ")\n",
    "    tuner.search(real_images_data, validation_data=(sketches_data, sketches_data), epochs=100)\n",
    "\n",
    "# Retrieve the best hyperparameters found during the search\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Best hyperparameters: {best_hps}\")\n",
    "\n",
    "# Retrieve and save the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.save('best_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568f238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 01s]\n",
      "\n",
      "Best val_loss So Far: None\n",
      "Total elapsed time: 00h 00m 03s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "leaky_relu        |leaky_relu        |activation\n",
      "0.0001            |0.0001            |learning_rate\n",
      "32                |32                |batch_size\n",
      "128               |256               |units\n",
      "3                 |3                 |hidden_layers\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\__autograph_generated_fileryz5i33v.py\", line 18, in tf__train_function\n",
      "    raise\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 12288)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\__autograph_generated_fileryz5i33v.py\", line 18, in tf__train_function\n    raise\nValueError: in user code:\n\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 12288)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 100\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Define the search strategy and start the hyperparameter search\u001b[39;00m\n\u001b[0;32m     91\u001b[0m tuner \u001b[38;5;241m=\u001b[39m RandomSearch(\n\u001b[0;32m     92\u001b[0m     build_gan,\n\u001b[0;32m     93\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Overwrite existing results\u001b[39;00m\n\u001b[0;32m     99\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(real_images_data, validation_data\u001b[38;5;241m=\u001b[39m(sketches_data, sketches_data), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Retrieve the best hyperparameters found during the search\u001b[39;00m\n\u001b[0;32m    103\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mend_trial(trial)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[1;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[0;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:586\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_consecutive_failures()\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:543\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    541\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    547\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\paivi\\AppData\\Local\\Temp\\__autograph_generated_fileryz5i33v.py\", line 18, in tf__train_function\n    raise\nValueError: in user code:\n\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\paivi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 100), found shape=(None, 12288)\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperParameters  # Import HyperParameters module\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to preprocess data\n",
    "# Function to preprocess data\n",
    "def preprocess_data(images_data, target_size):\n",
    "    # Resize images to target size and flatten\n",
    "    resized_images = [cv2.resize(img, target_size).flatten() for img in images_data]\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized_images = np.array(resized_images) / 255.0\n",
    "    return normalized_images\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_data(real_images_dir, sketches_dir, batch_size, image_size):\n",
    "    # Load real images\n",
    "    real_images_paths = [os.path.join(real_images_dir, filename) for filename in os.listdir(real_images_dir)]\n",
    "    real_images = [cv2.imread(path) for path in real_images_paths]\n",
    "\n",
    "    # Load sketches\n",
    "    sketches_paths = [os.path.join(sketches_dir, filename) for filename in os.listdir(sketches_dir)]\n",
    "    sketches = [cv2.imread(path) for path in sketches_paths]\n",
    "\n",
    "    print(\"Number of real images:\", len(real_images))\n",
    "    print(\"Number of sketches:\", len(sketches))\n",
    "\n",
    "    return real_images, sketches\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(hp):\n",
    "    # Define hyperparameters to tune\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'leaky_relu'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    hp_batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n",
    "    hp_units = hp.Choice('units', values=[128, 256, 512])\n",
    "    hp_hidden_layers = hp.Int('hidden_layers', min_value=1, max_value=3)\n",
    "\n",
    "    # Build the generator\n",
    "    generator = models.Sequential()\n",
    "    generator.add(layers.Dense(hp_units, input_shape=(100,), activation=hp_activation))\n",
    "    generator.add(layers.BatchNormalization())\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        generator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "        generator.add(layers.BatchNormalization())\n",
    "    generator.add(layers.Dense(64*64*3, activation='sigmoid'))  # Output shape should match the flattened image size\n",
    "\n",
    "    # Reshape the output to match image dimensions\n",
    "    generator.add(layers.Reshape((64, 64, 3)))  # Adjust dimensions as needed\n",
    "\n",
    "    # Build the discriminator\n",
    "    discriminator = models.Sequential()\n",
    "    discriminator.add(layers.Flatten(input_shape=(64, 64, 3)))  # Adjust input shape accordingly\n",
    "    discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    for _ in range(hp_hidden_layers - 1):\n",
    "        discriminator.add(layers.Dense(hp_units, activation=hp_activation))\n",
    "    discriminator.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "    # Compile GAN\n",
    "    gan_input = layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = models.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=hp_learning_rate, beta_1=0.5))\n",
    "\n",
    "    return gan\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    real_images_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Real\"\n",
    "    sketches_dir = r\"C:\\Users\\paivi\\Downloads\\Telegram Desktop\\Sketch_1\"\n",
    "    batch_size = 32\n",
    "    image_size = (64, 64)  # Adjust image size as needed\n",
    "    real_images, sketches = load_data(real_images_dir, sketches_dir, batch_size, image_size)\n",
    "\n",
    "    # Preprocess the loaded data\n",
    "    real_images_data = preprocess_data(real_images, target_size=(64, 64))\n",
    "    sketches_data = preprocess_data(sketches, target_size=(64, 64))\n",
    "\n",
    "    # Define the search strategy and start the hyperparameter search\n",
    "    tuner = RandomSearch(\n",
    "        build_gan,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        directory='my_dir',\n",
    "        project_name='gan_hyperparameter_tuning',\n",
    "        overwrite=True  # Overwrite existing results\n",
    "    )\n",
    "    tuner.search(real_images_data, validation_data=(sketches_data, sketches_data), epochs=100)\n",
    "\n",
    "    # Retrieve the best hyperparameters found during the search\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(f\"Best hyperparameters: {best_hps}\")\n",
    "\n",
    "    # Retrieve and save the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.save('best_generator_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278caf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8014b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a880a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
